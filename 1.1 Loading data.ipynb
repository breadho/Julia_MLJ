{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/D0-loading/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/D0-loading/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `D:\\JULIA\\6_ML_with_Julia\\D0-loading`\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "`StatsBase` is a direct dependency, but does not appear in the manifest. If you intend `StatsBase` to be a direct dependency, run `Pkg.resolve()` to populate the manifest. Otherwise, remove `StatsBase` with `Pkg.rm(\"StatsBase\")`. Finally, run `Pkg.instantiate()` again.",
     "output_type": "error",
     "traceback": [
      "`StatsBase` is a direct dependency, but does not appear in the manifest. If you intend `StatsBase` to be a direct dependency, run `Pkg.resolve()` to populate the manifest. Otherwise, remove `StatsBase` with `Pkg.rm(\"StatsBase\")`. Finally, run `Pkg.instantiate()` again.",
      "",
      "Stacktrace:",
      " [1] pkgerror(::String, ::Vararg{String})",
      "   @ Pkg.Types C:\\Program Files\\JULIA\\Julia-1.7.3\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\Types.jl:68",
      " [2] instantiate(ctx::Pkg.Types.Context; manifest::Nothing, update_registry::Bool, verbose::Bool, platform::Base.BinaryPlatforms.Platform, allow_build::Bool, allow_autoprecomp::Bool, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "   @ Pkg.API C:\\Program Files\\JULIA\\Julia-1.7.3\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\API.jl:1453",
      " [3] instantiate",
      "   @ C:\\Program Files\\JULIA\\Julia-1.7.3\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\API.jl:1422 [inlined]",
      " [4] #instantiate#276",
      "   @ C:\\Program Files\\JULIA\\Julia-1.7.3\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\API.jl:1418 [inlined]",
      " [5] instantiate()",
      "   @ Pkg.API C:\\Program Files\\JULIA\\Julia-1.7.3\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\API.jl:1418",
      " [6] top-level scope",
      "   @ In[1]:1",
      " [7] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"D:/JULIA/6_ML_with_Julia/D0-loading\") ; Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `D:\\JULIA\\6_ML_with_Julia\\D0-loading\\Project.toml`\n",
      " \u001b[90m [2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.33.21\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `D:\\JULIA\\6_ML_with_Julia\\D0-loading\\Manifest.toml`\n",
      " \u001b[90m [d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v1.15.6\u001b[39m\n",
      " \u001b[90m [9e997f8a] \u001b[39m\u001b[92m+ ChangesOfVariables v0.1.4\u001b[39m\n",
      " \u001b[90m [ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.1\u001b[39m\n",
      " \u001b[90m [3587e190] \u001b[39m\u001b[92m+ InverseFunctions v0.1.8\u001b[39m\n",
      " \u001b[90m [92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.1.1\u001b[39m\n",
      " \u001b[90m [2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.18\u001b[39m\n",
      " \u001b[90m [82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.5.0\u001b[39m\n",
      " \u001b[90m [2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.33.21\u001b[39m\n",
      " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `D:\\JULIA\\6_ML_with_Julia\\D0-loading`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"D:/JULIA/6_ML_with_Julia/D0-loading\") ; Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short tutorial we discuss two ways to easily load data in Julia:\n",
    "\n",
    "1. loading a standard dataset via `RDatasets.jl`,\n",
    "1. loading a local file with `CSV.jl`,\n",
    "\n",
    "## Using RDatasets\n",
    "\n",
    "The package [RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl) provides access to most of the many datasets listed on [this page](http://vincentarelbundock.github.io/Rdatasets/datasets.html).\n",
    "These are well known, standard datasets that can be used to get started with data processing and classical machine learning such as for instance `iris`, `crabs`, `Boston`, etc.\n",
    "\n",
    "To load such a dataset, you will need to specify which R package it belongs to as well as its name; for instance `Boston` is part of `MASS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RDatasets\n",
    "import DataFrames\n",
    "\n",
    "boston = dataset(\"MASS\", \"Boston\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that `Boston` is part of `MASS` is clearly indicated on the [list](http://vincentarelbundock.github.io/Rdatasets/datasets.html) linked to earlier.\n",
    "While it can be a bit slow, loading a dataset via RDatasets is very simple and convenient as you don't have to  worry about setting the names of columns etc.\n",
    "\n",
    "The `dataset` function returns a `DataFrame` object from the [DataFrames.jl](https://github.com/JuliaData/DataFrames.jl) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a short introduction to DataFrame objects, see [this tutorial](/data/dataframe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CSV\n",
    "\n",
    "The package [CSV.jl](https://github.com/JuliaData/CSV.jl) offers a powerful way to read arbitrary CSV files efficiently.\n",
    "In particular the `CSV.read` function allows to read a file and return a DataFrame.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "Let's say you have a file `foo.csv` at some path `fpath=joinpath(\"data\", \"foo.csv\")` with the content\n",
    "\n",
    "```\n",
    "col1,col2,col3,col4,col5,col6,col7,col8\n",
    ",1,1.0,1,one,2019-01-01,2019-01-01T00:00:00,true\n",
    ",2,2.0,2,two,2019-01-02,2019-01-02T00:00:00,false\n",
    ",3,3.0,3.14,three,2019-01-03,2019-01-03T00:00:00,true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"\"\"\n",
    "col1, col2, col3, col4, col5, col6, col7, col8\n",
    ",1,1.0,1,one,2019-01-01,2019-01-01T00:00:00,true\n",
    ",2,2.0,2,two,2019-01-02,2019-01-02T00:00:00,false\n",
    ",3,3.0,3.14,three,2019-01-03,2019-01-03T00:00:00,true\n",
    "\"\"\"\n",
    "fpath, = mktemp()\n",
    "write(fpath, c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\jeffr\\\\AppData\\\\Local\\\\Temp\\\\jl_76A2.tmp\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"C:\\\\Users\\\\jeffr\\\\AppData\\\\Local\\\\Temp\\\\jl_8912.tmp\", IOStream(<file C:\\Users\\jeffr\\AppData\\Local\\Temp\\jl_8912.tmp>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mktemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mktemp()는 임시 경로와 input-output stream을 제공하는 함수로 Base.Filesystem에서 export됨 \n",
    "\n",
    "     - **```mktemp(parent=tempdir(); cleanup=true) -> (path, io)```**\n",
    "\n",
    "     - Return ```(path, io)```, where path is the path of a new temporary file in ```parent``` and io is an open file object for this path. The ```cleanup``` option controls whether the temporary file is automatically deleted when the process exits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read it with CSV using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>3 rows × 8 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>col1</th><th> col2</th><th> col3</th><th> col4</th><th> col5</th><th> col6</th><th> col7</th><th> col8</th></tr><tr><th></th><th title=\"Missing\">Missing</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"String7\">String7</th><th title=\"Dates.Date\">Date</th><th title=\"Dates.DateTime\">DateTime</th><th title=\"Bool\">Bool</th></tr></thead><tbody><tr><th>1</th><td><em>missing</em></td><td>1</td><td>1.0</td><td>1.0</td><td>one</td><td>2019-01-01</td><td>2019-01-01T00:00:00</td><td>1</td></tr><tr><th>2</th><td><em>missing</em></td><td>2</td><td>2.0</td><td>2.0</td><td>two</td><td>2019-01-02</td><td>2019-01-02T00:00:00</td><td>0</td></tr><tr><th>3</th><td><em>missing</em></td><td>3</td><td>3.0</td><td>3.14</td><td>three</td><td>2019-01-03</td><td>2019-01-03T00:00:00</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& col1 &  col2 &  col3 &  col4 &  col5 &  col6 &  col7 &  col8\\\\\n",
       "\t\\hline\n",
       "\t& Missing & Int64 & Float64 & Float64 & String7 & Date & DateTime & Bool\\\\\n",
       "\t\\hline\n",
       "\t1 & \\emph{missing} & 1 & 1.0 & 1.0 & one & 2019-01-01 & 2019-01-01T00:00:00 & 1 \\\\\n",
       "\t2 & \\emph{missing} & 2 & 2.0 & 2.0 & two & 2019-01-02 & 2019-01-02T00:00:00 & 0 \\\\\n",
       "\t3 & \\emph{missing} & 3 & 3.0 & 3.14 & three & 2019-01-03 & 2019-01-03T00:00:00 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×8 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m col1    \u001b[0m\u001b[1m  col2 \u001b[0m\u001b[1m  col3   \u001b[0m\u001b[1m  col4   \u001b[0m\u001b[1m  col5   \u001b[0m\u001b[1m  col6      \u001b[0m\u001b[1m  col7           \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Missing \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Date       \u001b[0m\u001b[90m DateTime        \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │\u001b[90m missing \u001b[0m     1      1.0     1.0   one      2019-01-01  2019-01-01T00:00 ⋯\n",
       "   2 │\u001b[90m missing \u001b[0m     2      2.0     2.0   two      2019-01-02  2019-01-02T00:00\n",
       "   3 │\u001b[90m missing \u001b[0m     3      3.0     3.14  three    2019-01-03  2019-01-03T00:00\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "data = CSV.read(fpath, DataFrames.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use this `joinpath` for compatibility with  our system but you could pass any valid path on your system for instance `CSV.read(\"path/to/file.csv\")`.\n",
    "The data is also returned as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the useful arguments for `read` are:\n",
    "\n",
    "* `header=` to specify whether there's a header, or which line the header is on or to specify a full header yourself,\n",
    "* `skipto=` to specify how many rows to skip before starting to read the data,\n",
    "* `limit=` to specify a maximum number of rows to parse,\n",
    "* `missingstring=` to specify a string or vector of strings that should be parsed as missing values,\n",
    "* `delim=','` a char or string to specify how columns are separated.\n",
    "\n",
    "For more details see `?CSV.File`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "CSV.File(input; kwargs...) => CSV.File\n",
       "\\end{verbatim}\n",
       "Read a UTF-8 CSV input and return a \\texttt{CSV.File} object, which is like a lightweight table/dataframe, allowing dot-access to columns and iterating rows. Satisfies the Tables.jl interface, so can be passed to any valid sink, yet to avoid unnecessary copies of data, use \\texttt{CSV.read(input, sink; kwargs...)} instead if the \\texttt{CSV.File} intermediate object isn't needed.\n",
       "\n",
       "The \\href{@ref input}{\\texttt{input}} argument can be one of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item filename given as a string or FilePaths.jl type\n",
       "\n",
       "\n",
       "\\item a \\texttt{Vector\\{UInt8\\}} or \\texttt{SubArray\\{UInt8, 1, Vector\\{UInt8\\}\\}} byte buffer\n",
       "\n",
       "\n",
       "\\item a \\texttt{CodeUnits} object, which wraps a \\texttt{String}, like \\texttt{codeunits(str)}\n",
       "\n",
       "\n",
       "\\item a csv-formatted string can also be passed like \\texttt{IOBuffer(str)}\n",
       "\n",
       "\n",
       "\\item a \\texttt{Cmd} or other \\texttt{IO}\n",
       "\n",
       "\n",
       "\\item a gzipped file (or gzipped data in any of the above), which will automatically be decompressed for parsing\n",
       "\n",
       "\n",
       "\\item a \\texttt{Vector} of any of the above, which will parse and vertically concatenate each source, returning a single, \"long\" \\texttt{CSV.File}\n",
       "\n",
       "\\end{itemize}\n",
       "To read a csv file from a url, use the Downloads.jl stdlib or HTTP.jl package, where the resulting downloaded tempfile or \\texttt{HTTP.Response} body can be passed like:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Downloads, CSV\n",
       "f = CSV.File(Downloads.download(url))\n",
       "\n",
       "# or\n",
       "\n",
       "using HTTP, CSV\n",
       "f = CSV.File(HTTP.get(url).body)\n",
       "\\end{verbatim}\n",
       "Opens the file or files and uses passed arguments to detect the number of columns and column types, unless column types are provided manually via the \\texttt{types} keyword argument. Note that passing column types manually can slightly increase performance for each column type provided (column types can be given as a \\texttt{Vector} for all columns, or specified per column via name or index in a \\texttt{Dict}).\n",
       "\n",
       "When a \\texttt{Vector} of inputs is provided, the column names and types of each separate file/input must match to be vertically concatenated. Separate threads will be used to parse each input, which will each parse their input using just the single thread. The results of all threads are then vertically concatenated using \\texttt{ChainedVector}s to lazily concatenate each thread's columns.\n",
       "\n",
       "For text encodings other than UTF-8, load the \\href{https://github.com/JuliaStrings/StringEncodings.jl}{StringEncodings.jl} package and call e.g. \\texttt{CSV.File(open(read, input, enc\"ISO-8859-1\"))}.\n",
       "\n",
       "The returned \\texttt{CSV.File} object supports the \\href{https://github.com/JuliaData/Tables.jl}{Tables.jl} interface and can iterate \\texttt{CSV.Row}s. \\texttt{CSV.Row} supports \\texttt{propertynames} and \\texttt{getproperty} to access individual row values. \\texttt{CSV.File} also supports entire column access like a \\texttt{DataFrame} via direct property access on the file object, like \\texttt{f = CSV.File(file); f.col1}. Or by getindex access with column names, like \\texttt{f[:col1]} or \\texttt{f[\"col1\"]}. The returned columns are \\texttt{AbstractArray} subtypes, including: \\texttt{SentinelVector} (for integers), regular \\texttt{Vector}, \\texttt{PooledVector} for pooled columns, \\texttt{MissingVector} for columns of all \\texttt{missing} values, \\texttt{PosLenStringVector} when \\texttt{stringtype=PosLenString} is passed, and \\texttt{ChainedVector} will chain one of the previous array types together for data inputs that use multiple threads to parse (each thread parses a single \"chain\" of the input). Note that duplicate column names will be detected and adjusted to ensure uniqueness (duplicate column name \\texttt{a} will become \\texttt{a\\_1}). For example, one could iterate over a csv file with column names \\texttt{a}, \\texttt{b}, and \\texttt{c} by doing:\n",
       "\n",
       "\\begin{verbatim}\n",
       "for row in CSV.File(file)\n",
       "    println(\"a=$(row.a), b=$(row.b), c=$(row.c)\")\n",
       "end\n",
       "\\end{verbatim}\n",
       "By supporting the Tables.jl interface, a \\texttt{CSV.File} can also be a table input to any other table sink function. Like:\n",
       "\n",
       "\\begin{verbatim}\n",
       "# materialize a csv file as a DataFrame, copying columns from CSV.File\n",
       "df = CSV.File(file) |> DataFrame\n",
       "\n",
       "# to avoid making a copy of parsed columns, use CSV.read\n",
       "df = CSV.read(file, DataFrame)\n",
       "\n",
       "# load a csv file directly into an sqlite database table\n",
       "db = SQLite.DB()\n",
       "tbl = CSV.File(file) |> SQLite.load!(db, \"sqlite_table\")\n",
       "\\end{verbatim}\n",
       "\\section{Arguments}\n",
       "\\subsection{File layout options:}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{header=1}: how column names should be determined; if given as an \\texttt{Integer}, indicates the row to parse for column names; as an \\texttt{AbstractVector\\{<:Integer\\}}, indicates a set of rows to be concatenated together as column names; \\texttt{Vector\\{Symbol\\}} or \\texttt{Vector\\{String\\}} give column names explicitly (should match \\# of columns in dataset); if a dataset doesn't have column names, either provide them as a \\texttt{Vector}, or set \\texttt{header=0} or \\texttt{header=false} and column names will be auto-generated (\\texttt{Column1}, \\texttt{Column2}, etc.). Note that if a row number header and \\texttt{comment} or \\texttt{ignoreemptyrows} are provided, the header row will be the first non-commented/non-empty row \\emph{after} the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.\n",
       "\n",
       "\n",
       "\\item \\texttt{normalizenames::Bool=false}: whether column names should be \"normalized\" into valid Julia identifier symbols; useful when using the \\texttt{tbl.col1} \\texttt{getproperty} syntax or iterating rows and accessing column values of a row via \\texttt{getproperty} (e.g. \\texttt{row.col1})\n",
       "\n",
       "\n",
       "\\item \\texttt{skipto::Integer}: specifies the row where the data starts in the csv file; by default, the next row after the \\texttt{header} row(s) is used. If \\texttt{header=0}, then the 1st row is assumed to be the start of data; providing a \\texttt{skipto} argument does \\emph{not} affect the \\texttt{header} argument. Note that if a row number \\texttt{skipto} and \\texttt{comment} or \\texttt{ignoreemptyrows} are provided, the data row will be the first non-commented/non-empty row \\emph{after} the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.\n",
       "\n",
       "\n",
       "\\item \\texttt{footerskip::Integer}: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the \\texttt{comment} keyword argument) \\emph{do not} count towards the row number provided for \\texttt{footerskip}, they are completely ignored by the parser\n",
       "\n",
       "\n",
       "\\item \\texttt{transpose::Bool}: read a csv file \"transposed\", i.e. each column is parsed as a row\n",
       "\n",
       "\n",
       "\\item \\texttt{comment::String}: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or \\texttt{skipto} and \\texttt{comment} are provided, the header/data row will be the first non-commented/non-empty row \\emph{after} the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.\n",
       "\n",
       "\n",
       "\\item \\texttt{ignoreemptyrows::Bool=true}: whether empty rows in a file should be ignored (if \\texttt{false}, each column will be assigned \\texttt{missing} for that empty row)\n",
       "\n",
       "\n",
       "\\item \\texttt{select}: an \\texttt{AbstractVector} of \\texttt{Integer}, \\texttt{Symbol}, \\texttt{String}, or \\texttt{Bool}, or a \"selector\" function of the form \\texttt{(i, name) -> keep::Bool}; only columns in the collection or for which the selector function returns \\texttt{true} will be parsed and accessible in the resulting \\texttt{CSV.File}. Invalid values in \\texttt{select} are ignored.\n",
       "\n",
       "\n",
       "\\item \\texttt{drop}: inverse of \\texttt{select}; an \\texttt{AbstractVector} of \\texttt{Integer}, \\texttt{Symbol}, \\texttt{String}, or \\texttt{Bool}, or a \"drop\" function of the form \\texttt{(i, name) -> drop::Bool}; columns in the collection or for which the drop function returns \\texttt{true} will ignored in the resulting \\texttt{CSV.File}. Invalid values in \\texttt{drop} are ignored.\n",
       "\n",
       "\n",
       "\\item \\texttt{limit}: an \\texttt{Integer} to indicate a limited number of rows to parse in a csv file; use in combination with \\texttt{skipto} to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the \\texttt{limit} argument may not result in an exact \\# of rows parsed; use \\texttt{threaded=false} to ensure an exact limit if necessary\n",
       "\n",
       "\n",
       "\\item \\texttt{buffer\\_in\\_memory}: a \\texttt{Bool}, default \\texttt{false}, which controls whether a \\texttt{Cmd}, \\texttt{IO}, or gzipped source will be read/decompressed in memory vs. using a temporary file.\n",
       "\n",
       "\n",
       "\\item \\texttt{ntasks::Integer=Threads.nthreads()}: [not applicable to \\texttt{CSV.Rows}] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the \\# of threads Julia was started with (i.e. \\texttt{JULIA\\_NUM\\_THREADS} environment variable or \\texttt{julia -t N}); setting \\texttt{ntasks=1} will avoid any calls to \\texttt{Threads.@spawn} and just read the file serially on the main thread; a single thread will also be used for smaller files by default (< 5\\_000 cells)\n",
       "\n",
       "\n",
       "\\item \\texttt{rows\\_to\\_check::Integer=30}: [not applicable to \\texttt{CSV.Rows}] a multithreaded parsed file will be split up into \\texttt{ntasks} \\# of equal chunks; \\texttt{rows\\_to\\_check} controls the \\# of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, \\texttt{lines\\_to\\_check} may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows\n",
       "\n",
       "\n",
       "\\item \\texttt{source}: [only applicable for vector of inputs to \\texttt{CSV.File}] a \\texttt{Symbol}, \\texttt{String}, or \\texttt{Pair} of \\texttt{Symbol} or \\texttt{String} to \\texttt{Vector}. As a single \\texttt{Symbol} or \\texttt{String}, provides the column name that will be added to the parsed columns, the values of the column will be the input \"name\" (usually file name) of the input from whence the value was parsed. As a \\texttt{Pair}, the 2nd part of the pair should be a \\texttt{Vector} of values matching the length of the \\# of inputs, where each value will be used instead of the input name for that inputs values in the auto-added column.\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Parsing options:}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{missingstring}: either a \\texttt{nothing}, \\texttt{String}, or \\texttt{Vector\\{String\\}} to use as sentinel values that will be parsed as \\texttt{missing}; if \\texttt{nothing} is passed, no sentinel/missing values will be parsed; by default, \\texttt{missingstring=\"\"}, which means only an empty field (two consecutive delimiters) is considered \\texttt{missing}\n",
       "\n",
       "\n",
       "\\item \\texttt{delim=','}: a \\texttt{Char} or \\texttt{String} that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file\n",
       "\n",
       "\n",
       "\\item \\texttt{ignorerepeated::Bool=false}: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells\n",
       "\n",
       "\n",
       "\\item \\texttt{quoted::Bool=true}: whether parsing should check for \\texttt{quotechar} at the start/end of cells\n",
       "\n",
       "\n",
       "\\item \\texttt{quotechar='\"'}, \\texttt{openquotechar}, \\texttt{closequotechar}: a \\texttt{Char} (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters\n",
       "\n",
       "\n",
       "\\item \\texttt{escapechar='\"'}: the \\texttt{Char} used to escape quote characters in a quoted field\n",
       "\n",
       "\n",
       "\\item \\texttt{dateformat::Union\\{String, Dates.DateFormat, Nothing, AbstractDict\\}}: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an \\texttt{AbstractDict}, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index \\texttt{Int}, or name \\texttt{Symbol} or \\texttt{String} to the format string for that column.\n",
       "\n",
       "\n",
       "\\item \\texttt{decimal='.'}: a \\texttt{Char} indicating how decimals are separated in floats, i.e. \\texttt{3.14} uses \\texttt{'.'}, or \\texttt{3,14} uses a comma \\texttt{','}\n",
       "\n",
       "\n",
       "\\item \\texttt{truestrings}, \\texttt{falsestrings}: \\texttt{Vector\\{String\\}}s that indicate how \\texttt{true} or \\texttt{false} values are represented; by default \\texttt{\"true\", \"True\", \"TRUE\", \"T\", \"1\"} are used to detect \\texttt{true} and \\texttt{\"false\", \"False\", \"FALSE\", \"F\", \"0\"} are used to detect \\texttt{false}; note that columns with only \\texttt{1} and \\texttt{0} values will default to \\texttt{Int64} column type unless explicitly requested to be \\texttt{Bool} via \\texttt{types} keyword argument\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Column Type Options:}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{types}: a single \\texttt{Type}, \\texttt{AbstractVector} or \\texttt{AbstractDict} of types, or a function of the form \\texttt{(i, name) -> Union\\{T, Nothing\\}} to be used for column types; if a single \\texttt{Type} is provided, \\emph{all} columns will be parsed with that single type; an \\texttt{AbstractDict} can map column index \\texttt{Integer}, or name \\texttt{Symbol} or \\texttt{String} to type for a column, i.e. \\texttt{Dict(1=>Float64)} will set the first column as a \\texttt{Float64}, \\texttt{Dict(:column1=>Float64)} will set the column named \\texttt{column1} to \\texttt{Float64} and, \\texttt{Dict(\"column1\"=>Float64)} will set the \\texttt{column1} to \\texttt{Float64}; if a \\texttt{Vector} is provided, it must match the \\# of columns provided or detected in \\texttt{header}. If a function is provided, it takes a column index and name as arguments, and should return the desired column type for the column, or \\texttt{nothing} to signal the column's type should be detected while parsing.\n",
       "\n",
       "\n",
       "\\item \\texttt{typemap::Dict\\{Type, Type\\}}: a mapping of a type that should be replaced in every instance with another type, i.e. \\texttt{Dict(Float64=>String)} would change every detected \\texttt{Float64} column to be parsed as \\texttt{String}; only \"standard\" types are allowed to be mapped to another type, i.e. \\texttt{Int64}, \\texttt{Float64}, \\texttt{Date}, \\texttt{DateTime}, \\texttt{Time}, and \\texttt{Bool}. If a column of one of those types is \"detected\", it will be mapped to the specified type.\n",
       "\n",
       "\n",
       "\\item \\texttt{pool::Union\\{Bool, Real, AbstractVector, AbstractDict, Function\\}=0.25}: [not supported by \\texttt{CSV.Rows}] controls whether columns will be built as \\texttt{PooledArray}; if \\texttt{true}, all columns detected as \\texttt{String} will be pooled; alternatively, the proportion of unique values below which \\texttt{String} columns should be pooled (by default 0.25, meaning that if the \\# of unique strings in a column is under 25.0\\%, it will be pooled); if an \\texttt{AbstractVector}, each element should be \\texttt{Bool} or \\texttt{Real} and the \\# of elements should match the \\# of columns in the dataset; if an \\texttt{AbstractDict}, a \\texttt{Bool} or \\texttt{Real} value can be provided for individual columns where the dict key is given as column index \\texttt{Integer}, or column name as \\texttt{Symbol} or \\texttt{String}. If a function is provided, it should take a column index and name as 2 arguments, and return a \\texttt{Bool}, \\texttt{Float64}, or \\texttt{nothing} for each column.\n",
       "\n",
       "\n",
       "\\item \\texttt{downcast::Bool=false}: controls whether columns detected as \\texttt{Int64} will be \"downcast\" to the smallest possible integer type like \\texttt{Int8}, \\texttt{Int16}, \\texttt{Int32}, etc.\n",
       "\n",
       "\n",
       "\\item \\texttt{stringtype=InlineStrings.InlineString}: controls how detected string columns will ultimately be returned; default is \\texttt{InlineString}, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to \\texttt{String}. If \\texttt{String} is passed, all string columns will just be normal \\texttt{String} values. If \\texttt{PosLenString} is passed, string columns will be returned as \\texttt{PosLenStringVector}, which is a special \"lazy\" \\texttt{AbstractVector} that acts as a \"view\" into the original file data. This can lead to the most efficient parsing times, but note that the \"view\" nature of \\texttt{PosLenStringVector} makes it read-only, so operations like \\texttt{push!}, \\texttt{append!}, or \\texttt{setindex!} are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail\n",
       "\n",
       "\n",
       "\\item \\texttt{strict::Bool=false}: whether invalid values should throw a parsing error or be replaced with \\texttt{missing}\n",
       "\n",
       "\n",
       "\\item \\texttt{silencewarnings::Bool=false}: if \\texttt{strict=false}, whether invalid value warnings should be silenced\n",
       "\n",
       "\n",
       "\\item \\texttt{maxwarnings::Int=100}: if more than \\texttt{maxwarnings} number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to \\texttt{maxwarnings}\n",
       "\n",
       "\n",
       "\\item \\texttt{debug::Bool=false}: passing \\texttt{true} will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed\n",
       "\n",
       "\n",
       "\\item \\texttt{validate::Bool=true}: whether or not to validate that columns specified in the \\texttt{types}, \\texttt{dateformat} and \\texttt{pool} keywords are actually found in the data. If \\texttt{false} no validation is done, meaning no error will be thrown if \\texttt{types}/\\texttt{dateformat}/\\texttt{pool} specify settings for columns not actually found in the data.\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Iteration options:}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{reusebuffer=false}: [only supported by \\texttt{CSV.Rows}] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it's not safe to use this option if doing \\texttt{collect(CSV.Rows(file))} because only current iterated row is \"valid\")\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "CSV.File(input; kwargs...) => CSV.File\n",
       "```\n",
       "\n",
       "Read a UTF-8 CSV input and return a `CSV.File` object, which is like a lightweight table/dataframe, allowing dot-access to columns and iterating rows. Satisfies the Tables.jl interface, so can be passed to any valid sink, yet to avoid unnecessary copies of data, use `CSV.read(input, sink; kwargs...)` instead if the `CSV.File` intermediate object isn't needed.\n",
       "\n",
       "The [`input`](@ref input) argument can be one of:\n",
       "\n",
       "  * filename given as a string or FilePaths.jl type\n",
       "  * a `Vector{UInt8}` or `SubArray{UInt8, 1, Vector{UInt8}}` byte buffer\n",
       "  * a `CodeUnits` object, which wraps a `String`, like `codeunits(str)`\n",
       "  * a csv-formatted string can also be passed like `IOBuffer(str)`\n",
       "  * a `Cmd` or other `IO`\n",
       "  * a gzipped file (or gzipped data in any of the above), which will automatically be decompressed for parsing\n",
       "  * a `Vector` of any of the above, which will parse and vertically concatenate each source, returning a single, \"long\" `CSV.File`\n",
       "\n",
       "To read a csv file from a url, use the Downloads.jl stdlib or HTTP.jl package, where the resulting downloaded tempfile or `HTTP.Response` body can be passed like:\n",
       "\n",
       "```julia\n",
       "using Downloads, CSV\n",
       "f = CSV.File(Downloads.download(url))\n",
       "\n",
       "# or\n",
       "\n",
       "using HTTP, CSV\n",
       "f = CSV.File(HTTP.get(url).body)\n",
       "```\n",
       "\n",
       "Opens the file or files and uses passed arguments to detect the number of columns and column types, unless column types are provided manually via the `types` keyword argument. Note that passing column types manually can slightly increase performance for each column type provided (column types can be given as a `Vector` for all columns, or specified per column via name or index in a `Dict`).\n",
       "\n",
       "When a `Vector` of inputs is provided, the column names and types of each separate file/input must match to be vertically concatenated. Separate threads will be used to parse each input, which will each parse their input using just the single thread. The results of all threads are then vertically concatenated using `ChainedVector`s to lazily concatenate each thread's columns.\n",
       "\n",
       "For text encodings other than UTF-8, load the [StringEncodings.jl](https://github.com/JuliaStrings/StringEncodings.jl) package and call e.g. `CSV.File(open(read, input, enc\"ISO-8859-1\"))`.\n",
       "\n",
       "The returned `CSV.File` object supports the [Tables.jl](https://github.com/JuliaData/Tables.jl) interface and can iterate `CSV.Row`s. `CSV.Row` supports `propertynames` and `getproperty` to access individual row values. `CSV.File` also supports entire column access like a `DataFrame` via direct property access on the file object, like `f = CSV.File(file); f.col1`. Or by getindex access with column names, like `f[:col1]` or `f[\"col1\"]`. The returned columns are `AbstractArray` subtypes, including: `SentinelVector` (for integers), regular `Vector`, `PooledVector` for pooled columns, `MissingVector` for columns of all `missing` values, `PosLenStringVector` when `stringtype=PosLenString` is passed, and `ChainedVector` will chain one of the previous array types together for data inputs that use multiple threads to parse (each thread parses a single \"chain\" of the input). Note that duplicate column names will be detected and adjusted to ensure uniqueness (duplicate column name `a` will become `a_1`). For example, one could iterate over a csv file with column names `a`, `b`, and `c` by doing:\n",
       "\n",
       "```julia\n",
       "for row in CSV.File(file)\n",
       "    println(\"a=$(row.a), b=$(row.b), c=$(row.c)\")\n",
       "end\n",
       "```\n",
       "\n",
       "By supporting the Tables.jl interface, a `CSV.File` can also be a table input to any other table sink function. Like:\n",
       "\n",
       "```julia\n",
       "# materialize a csv file as a DataFrame, copying columns from CSV.File\n",
       "df = CSV.File(file) |> DataFrame\n",
       "\n",
       "# to avoid making a copy of parsed columns, use CSV.read\n",
       "df = CSV.read(file, DataFrame)\n",
       "\n",
       "# load a csv file directly into an sqlite database table\n",
       "db = SQLite.DB()\n",
       "tbl = CSV.File(file) |> SQLite.load!(db, \"sqlite_table\")\n",
       "```\n",
       "\n",
       "# Arguments\n",
       "\n",
       "## File layout options:\n",
       "\n",
       "  * `header=1`: how column names should be determined; if given as an `Integer`, indicates the row to parse for column names; as an `AbstractVector{<:Integer}`, indicates a set of rows to be concatenated together as column names; `Vector{Symbol}` or `Vector{String}` give column names explicitly (should match # of columns in dataset); if a dataset doesn't have column names, either provide them as a `Vector`, or set `header=0` or `header=false` and column names will be auto-generated (`Column1`, `Column2`, etc.). Note that if a row number header and `comment` or `ignoreemptyrows` are provided, the header row will be the first non-commented/non-empty row *after* the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.\n",
       "  * `normalizenames::Bool=false`: whether column names should be \"normalized\" into valid Julia identifier symbols; useful when using the `tbl.col1` `getproperty` syntax or iterating rows and accessing column values of a row via `getproperty` (e.g. `row.col1`)\n",
       "  * `skipto::Integer`: specifies the row where the data starts in the csv file; by default, the next row after the `header` row(s) is used. If `header=0`, then the 1st row is assumed to be the start of data; providing a `skipto` argument does *not* affect the `header` argument. Note that if a row number `skipto` and `comment` or `ignoreemptyrows` are provided, the data row will be the first non-commented/non-empty row *after* the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.\n",
       "  * `footerskip::Integer`: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the `comment` keyword argument) *do not* count towards the row number provided for `footerskip`, they are completely ignored by the parser\n",
       "  * `transpose::Bool`: read a csv file \"transposed\", i.e. each column is parsed as a row\n",
       "  * `comment::String`: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or `skipto` and `comment` are provided, the header/data row will be the first non-commented/non-empty row *after* the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.\n",
       "  * `ignoreemptyrows::Bool=true`: whether empty rows in a file should be ignored (if `false`, each column will be assigned `missing` for that empty row)\n",
       "  * `select`: an `AbstractVector` of `Integer`, `Symbol`, `String`, or `Bool`, or a \"selector\" function of the form `(i, name) -> keep::Bool`; only columns in the collection or for which the selector function returns `true` will be parsed and accessible in the resulting `CSV.File`. Invalid values in `select` are ignored.\n",
       "  * `drop`: inverse of `select`; an `AbstractVector` of `Integer`, `Symbol`, `String`, or `Bool`, or a \"drop\" function of the form `(i, name) -> drop::Bool`; columns in the collection or for which the drop function returns `true` will ignored in the resulting `CSV.File`. Invalid values in `drop` are ignored.\n",
       "  * `limit`: an `Integer` to indicate a limited number of rows to parse in a csv file; use in combination with `skipto` to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the `limit` argument may not result in an exact # of rows parsed; use `threaded=false` to ensure an exact limit if necessary\n",
       "  * `buffer_in_memory`: a `Bool`, default `false`, which controls whether a `Cmd`, `IO`, or gzipped source will be read/decompressed in memory vs. using a temporary file.\n",
       "  * `ntasks::Integer=Threads.nthreads()`: [not applicable to `CSV.Rows`] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the # of threads Julia was started with (i.e. `JULIA_NUM_THREADS` environment variable or `julia -t N`); setting `ntasks=1` will avoid any calls to `Threads.@spawn` and just read the file serially on the main thread; a single thread will also be used for smaller files by default (< 5_000 cells)\n",
       "  * `rows_to_check::Integer=30`: [not applicable to `CSV.Rows`] a multithreaded parsed file will be split up into `ntasks` # of equal chunks; `rows_to_check` controls the # of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, `lines_to_check` may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows\n",
       "  * `source`: [only applicable for vector of inputs to `CSV.File`] a `Symbol`, `String`, or `Pair` of `Symbol` or `String` to `Vector`. As a single `Symbol` or `String`, provides the column name that will be added to the parsed columns, the values of the column will be the input \"name\" (usually file name) of the input from whence the value was parsed. As a `Pair`, the 2nd part of the pair should be a `Vector` of values matching the length of the # of inputs, where each value will be used instead of the input name for that inputs values in the auto-added column.\n",
       "\n",
       "## Parsing options:\n",
       "\n",
       "  * `missingstring`: either a `nothing`, `String`, or `Vector{String}` to use as sentinel values that will be parsed as `missing`; if `nothing` is passed, no sentinel/missing values will be parsed; by default, `missingstring=\"\"`, which means only an empty field (two consecutive delimiters) is considered `missing`\n",
       "  * `delim=','`: a `Char` or `String` that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file\n",
       "  * `ignorerepeated::Bool=false`: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells\n",
       "  * `quoted::Bool=true`: whether parsing should check for `quotechar` at the start/end of cells\n",
       "  * `quotechar='\"'`, `openquotechar`, `closequotechar`: a `Char` (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters\n",
       "  * `escapechar='\"'`: the `Char` used to escape quote characters in a quoted field\n",
       "  * `dateformat::Union{String, Dates.DateFormat, Nothing, AbstractDict}`: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an `AbstractDict`, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index `Int`, or name `Symbol` or `String` to the format string for that column.\n",
       "  * `decimal='.'`: a `Char` indicating how decimals are separated in floats, i.e. `3.14` uses `'.'`, or `3,14` uses a comma `','`\n",
       "  * `truestrings`, `falsestrings`: `Vector{String}`s that indicate how `true` or `false` values are represented; by default `\"true\", \"True\", \"TRUE\", \"T\", \"1\"` are used to detect `true` and `\"false\", \"False\", \"FALSE\", \"F\", \"0\"` are used to detect `false`; note that columns with only `1` and `0` values will default to `Int64` column type unless explicitly requested to be `Bool` via `types` keyword argument\n",
       "\n",
       "## Column Type Options:\n",
       "\n",
       "  * `types`: a single `Type`, `AbstractVector` or `AbstractDict` of types, or a function of the form `(i, name) -> Union{T, Nothing}` to be used for column types; if a single `Type` is provided, *all* columns will be parsed with that single type; an `AbstractDict` can map column index `Integer`, or name `Symbol` or `String` to type for a column, i.e. `Dict(1=>Float64)` will set the first column as a `Float64`, `Dict(:column1=>Float64)` will set the column named `column1` to `Float64` and, `Dict(\"column1\"=>Float64)` will set the `column1` to `Float64`; if a `Vector` is provided, it must match the # of columns provided or detected in `header`. If a function is provided, it takes a column index and name as arguments, and should return the desired column type for the column, or `nothing` to signal the column's type should be detected while parsing.\n",
       "  * `typemap::Dict{Type, Type}`: a mapping of a type that should be replaced in every instance with another type, i.e. `Dict(Float64=>String)` would change every detected `Float64` column to be parsed as `String`; only \"standard\" types are allowed to be mapped to another type, i.e. `Int64`, `Float64`, `Date`, `DateTime`, `Time`, and `Bool`. If a column of one of those types is \"detected\", it will be mapped to the specified type.\n",
       "  * `pool::Union{Bool, Real, AbstractVector, AbstractDict, Function}=0.25`: [not supported by `CSV.Rows`] controls whether columns will be built as `PooledArray`; if `true`, all columns detected as `String` will be pooled; alternatively, the proportion of unique values below which `String` columns should be pooled (by default 0.25, meaning that if the # of unique strings in a column is under 25.0%, it will be pooled); if an `AbstractVector`, each element should be `Bool` or `Real` and the # of elements should match the # of columns in the dataset; if an `AbstractDict`, a `Bool` or `Real` value can be provided for individual columns where the dict key is given as column index `Integer`, or column name as `Symbol` or `String`. If a function is provided, it should take a column index and name as 2 arguments, and return a `Bool`, `Float64`, or `nothing` for each column.\n",
       "  * `downcast::Bool=false`: controls whether columns detected as `Int64` will be \"downcast\" to the smallest possible integer type like `Int8`, `Int16`, `Int32`, etc.\n",
       "  * `stringtype=InlineStrings.InlineString`: controls how detected string columns will ultimately be returned; default is `InlineString`, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to `String`. If `String` is passed, all string columns will just be normal `String` values. If `PosLenString` is passed, string columns will be returned as `PosLenStringVector`, which is a special \"lazy\" `AbstractVector` that acts as a \"view\" into the original file data. This can lead to the most efficient parsing times, but note that the \"view\" nature of `PosLenStringVector` makes it read-only, so operations like `push!`, `append!`, or `setindex!` are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail\n",
       "  * `strict::Bool=false`: whether invalid values should throw a parsing error or be replaced with `missing`\n",
       "  * `silencewarnings::Bool=false`: if `strict=false`, whether invalid value warnings should be silenced\n",
       "  * `maxwarnings::Int=100`: if more than `maxwarnings` number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to `maxwarnings`\n",
       "  * `debug::Bool=false`: passing `true` will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed\n",
       "  * `validate::Bool=true`: whether or not to validate that columns specified in the `types`, `dateformat` and `pool` keywords are actually found in the data. If `false` no validation is done, meaning no error will be thrown if `types`/`dateformat`/`pool` specify settings for columns not actually found in the data.\n",
       "\n",
       "## Iteration options:\n",
       "\n",
       "  * `reusebuffer=false`: [only supported by `CSV.Rows`] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it's not safe to use this option if doing `collect(CSV.Rows(file))` because only current iterated row is \"valid\")\n"
      ],
      "text/plain": [
       "\u001b[36m  CSV.File(input; kwargs...) => CSV.File\u001b[39m\n",
       "\n",
       "  Read a UTF-8 CSV input and return a \u001b[36mCSV.File\u001b[39m object, which is like a\n",
       "  lightweight table/dataframe, allowing dot-access to columns and iterating\n",
       "  rows. Satisfies the Tables.jl interface, so can be passed to any valid sink,\n",
       "  yet to avoid unnecessary copies of data, use \u001b[36mCSV.read(input, sink;\n",
       "  kwargs...)\u001b[39m instead if the \u001b[36mCSV.File\u001b[39m intermediate object isn't needed.\n",
       "\n",
       "  The \u001b[36minput\u001b[39m argument can be one of:\n",
       "\n",
       "    •  filename given as a string or FilePaths.jl type\n",
       "\n",
       "    •  a \u001b[36mVector{UInt8}\u001b[39m or \u001b[36mSubArray{UInt8, 1, Vector{UInt8}}\u001b[39m byte buffer\n",
       "\n",
       "    •  a \u001b[36mCodeUnits\u001b[39m object, which wraps a \u001b[36mString\u001b[39m, like \u001b[36mcodeunits(str)\u001b[39m\n",
       "\n",
       "    •  a csv-formatted string can also be passed like \u001b[36mIOBuffer(str)\u001b[39m\n",
       "\n",
       "    •  a \u001b[36mCmd\u001b[39m or other \u001b[36mIO\u001b[39m\n",
       "\n",
       "    •  a gzipped file (or gzipped data in any of the above), which will\n",
       "       automatically be decompressed for parsing\n",
       "\n",
       "    •  a \u001b[36mVector\u001b[39m of any of the above, which will parse and vertically\n",
       "       concatenate each source, returning a single, \"long\" \u001b[36mCSV.File\u001b[39m\n",
       "\n",
       "  To read a csv file from a url, use the Downloads.jl stdlib or HTTP.jl\n",
       "  package, where the resulting downloaded tempfile or \u001b[36mHTTP.Response\u001b[39m body can\n",
       "  be passed like:\n",
       "\n",
       "\u001b[36m  using Downloads, CSV\u001b[39m\n",
       "\u001b[36m  f = CSV.File(Downloads.download(url))\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # or\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  using HTTP, CSV\u001b[39m\n",
       "\u001b[36m  f = CSV.File(HTTP.get(url).body)\u001b[39m\n",
       "\n",
       "  Opens the file or files and uses passed arguments to detect the number of\n",
       "  columns and column types, unless column types are provided manually via the\n",
       "  \u001b[36mtypes\u001b[39m keyword argument. Note that passing column types manually can slightly\n",
       "  increase performance for each column type provided (column types can be\n",
       "  given as a \u001b[36mVector\u001b[39m for all columns, or specified per column via name or index\n",
       "  in a \u001b[36mDict\u001b[39m).\n",
       "\n",
       "  When a \u001b[36mVector\u001b[39m of inputs is provided, the column names and types of each\n",
       "  separate file/input must match to be vertically concatenated. Separate\n",
       "  threads will be used to parse each input, which will each parse their input\n",
       "  using just the single thread. The results of all threads are then vertically\n",
       "  concatenated using \u001b[36mChainedVector\u001b[39ms to lazily concatenate each thread's\n",
       "  columns.\n",
       "\n",
       "  For text encodings other than UTF-8, load the StringEncodings.jl\n",
       "  (https://github.com/JuliaStrings/StringEncodings.jl) package and call e.g.\n",
       "  \u001b[36mCSV.File(open(read, input, enc\"ISO-8859-1\"))\u001b[39m.\n",
       "\n",
       "  The returned \u001b[36mCSV.File\u001b[39m object supports the Tables.jl\n",
       "  (https://github.com/JuliaData/Tables.jl) interface and can iterate \u001b[36mCSV.Row\u001b[39ms.\n",
       "  \u001b[36mCSV.Row\u001b[39m supports \u001b[36mpropertynames\u001b[39m and \u001b[36mgetproperty\u001b[39m to access individual row\n",
       "  values. \u001b[36mCSV.File\u001b[39m also supports entire column access like a \u001b[36mDataFrame\u001b[39m via\n",
       "  direct property access on the file object, like \u001b[36mf = CSV.File(file); f.col1\u001b[39m.\n",
       "  Or by getindex access with column names, like \u001b[36mf[:col1]\u001b[39m or \u001b[36mf[\"col1\"]\u001b[39m. The\n",
       "  returned columns are \u001b[36mAbstractArray\u001b[39m subtypes, including: \u001b[36mSentinelVector\u001b[39m (for\n",
       "  integers), regular \u001b[36mVector\u001b[39m, \u001b[36mPooledVector\u001b[39m for pooled columns, \u001b[36mMissingVector\u001b[39m\n",
       "  for columns of all \u001b[36mmissing\u001b[39m values, \u001b[36mPosLenStringVector\u001b[39m when\n",
       "  \u001b[36mstringtype=PosLenString\u001b[39m is passed, and \u001b[36mChainedVector\u001b[39m will chain one of the\n",
       "  previous array types together for data inputs that use multiple threads to\n",
       "  parse (each thread parses a single \"chain\" of the input). Note that\n",
       "  duplicate column names will be detected and adjusted to ensure uniqueness\n",
       "  (duplicate column name \u001b[36ma\u001b[39m will become \u001b[36ma_1\u001b[39m). For example, one could iterate\n",
       "  over a csv file with column names \u001b[36ma\u001b[39m, \u001b[36mb\u001b[39m, and \u001b[36mc\u001b[39m by doing:\n",
       "\n",
       "\u001b[36m  for row in CSV.File(file)\u001b[39m\n",
       "\u001b[36m      println(\"a=$(row.a), b=$(row.b), c=$(row.c)\")\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\n",
       "  By supporting the Tables.jl interface, a \u001b[36mCSV.File\u001b[39m can also be a table input\n",
       "  to any other table sink function. Like:\n",
       "\n",
       "\u001b[36m  # materialize a csv file as a DataFrame, copying columns from CSV.File\u001b[39m\n",
       "\u001b[36m  df = CSV.File(file) |> DataFrame\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # to avoid making a copy of parsed columns, use CSV.read\u001b[39m\n",
       "\u001b[36m  df = CSV.read(file, DataFrame)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # load a csv file directly into an sqlite database table\u001b[39m\n",
       "\u001b[36m  db = SQLite.DB()\u001b[39m\n",
       "\u001b[36m  tbl = CSV.File(file) |> SQLite.load!(db, \"sqlite_table\")\u001b[39m\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  File layout options:\u001b[22m\n",
       "\u001b[1m  ======================\u001b[22m\n",
       "\n",
       "    •  \u001b[36mheader=1\u001b[39m: how column names should be determined; if given as an\n",
       "       \u001b[36mInteger\u001b[39m, indicates the row to parse for column names; as an\n",
       "       \u001b[36mAbstractVector{<:Integer}\u001b[39m, indicates a set of rows to be\n",
       "       concatenated together as column names; \u001b[36mVector{Symbol}\u001b[39m or\n",
       "       \u001b[36mVector{String}\u001b[39m give column names explicitly (should match # of\n",
       "       columns in dataset); if a dataset doesn't have column names,\n",
       "       either provide them as a \u001b[36mVector\u001b[39m, or set \u001b[36mheader=0\u001b[39m or \u001b[36mheader=false\u001b[39m\n",
       "       and column names will be auto-generated (\u001b[36mColumn1\u001b[39m, \u001b[36mColumn2\u001b[39m, etc.).\n",
       "       Note that if a row number header and \u001b[36mcomment\u001b[39m or \u001b[36mignoreemptyrows\u001b[39m\n",
       "       are provided, the header row will be the first\n",
       "       non-commented/non-empty row \u001b[4mafter\u001b[24m the row number, meaning if the\n",
       "       provided row number is a commented row, the header row will\n",
       "       actually be the next non-commented row.\n",
       "\n",
       "    •  \u001b[36mnormalizenames::Bool=false\u001b[39m: whether column names should be\n",
       "       \"normalized\" into valid Julia identifier symbols; useful when\n",
       "       using the \u001b[36mtbl.col1\u001b[39m \u001b[36mgetproperty\u001b[39m syntax or iterating rows and\n",
       "       accessing column values of a row via \u001b[36mgetproperty\u001b[39m (e.g. \u001b[36mrow.col1\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mskipto::Integer\u001b[39m: specifies the row where the data starts in the\n",
       "       csv file; by default, the next row after the \u001b[36mheader\u001b[39m row(s) is\n",
       "       used. If \u001b[36mheader=0\u001b[39m, then the 1st row is assumed to be the start of\n",
       "       data; providing a \u001b[36mskipto\u001b[39m argument does \u001b[4mnot\u001b[24m affect the \u001b[36mheader\u001b[39m\n",
       "       argument. Note that if a row number \u001b[36mskipto\u001b[39m and \u001b[36mcomment\u001b[39m or\n",
       "       \u001b[36mignoreemptyrows\u001b[39m are provided, the data row will be the first\n",
       "       non-commented/non-empty row \u001b[4mafter\u001b[24m the row number, meaning if the\n",
       "       provided row number is a commented row, the data row will actually\n",
       "       be the next non-commented row.\n",
       "\n",
       "    •  \u001b[36mfooterskip::Integer\u001b[39m: number of rows at the end of a file to skip\n",
       "       parsing. Do note that commented rows (see the \u001b[36mcomment\u001b[39m keyword\n",
       "       argument) \u001b[4mdo not\u001b[24m count towards the row number provided for\n",
       "       \u001b[36mfooterskip\u001b[39m, they are completely ignored by the parser\n",
       "\n",
       "    •  \u001b[36mtranspose::Bool\u001b[39m: read a csv file \"transposed\", i.e. each column is\n",
       "       parsed as a row\n",
       "\n",
       "    •  \u001b[36mcomment::String\u001b[39m: string that will cause rows that begin with it to\n",
       "       be skipped while parsing. Note that if a row number header or\n",
       "       \u001b[36mskipto\u001b[39m and \u001b[36mcomment\u001b[39m are provided, the header/data row will be the\n",
       "       first non-commented/non-empty row \u001b[4mafter\u001b[24m the row number, meaning if\n",
       "       the provided row number is a commented row, the header/data row\n",
       "       will actually be the next non-commented row.\n",
       "\n",
       "    •  \u001b[36mignoreemptyrows::Bool=true\u001b[39m: whether empty rows in a file should be\n",
       "       ignored (if \u001b[36mfalse\u001b[39m, each column will be assigned \u001b[36mmissing\u001b[39m for that\n",
       "       empty row)\n",
       "\n",
       "    •  \u001b[36mselect\u001b[39m: an \u001b[36mAbstractVector\u001b[39m of \u001b[36mInteger\u001b[39m, \u001b[36mSymbol\u001b[39m, \u001b[36mString\u001b[39m, or \u001b[36mBool\u001b[39m, or\n",
       "       a \"selector\" function of the form \u001b[36m(i, name) -> keep::Bool\u001b[39m; only\n",
       "       columns in the collection or for which the selector function\n",
       "       returns \u001b[36mtrue\u001b[39m will be parsed and accessible in the resulting\n",
       "       \u001b[36mCSV.File\u001b[39m. Invalid values in \u001b[36mselect\u001b[39m are ignored.\n",
       "\n",
       "    •  \u001b[36mdrop\u001b[39m: inverse of \u001b[36mselect\u001b[39m; an \u001b[36mAbstractVector\u001b[39m of \u001b[36mInteger\u001b[39m, \u001b[36mSymbol\u001b[39m,\n",
       "       \u001b[36mString\u001b[39m, or \u001b[36mBool\u001b[39m, or a \"drop\" function of the form \u001b[36m(i, name) ->\n",
       "       drop::Bool\u001b[39m; columns in the collection or for which the drop\n",
       "       function returns \u001b[36mtrue\u001b[39m will ignored in the resulting \u001b[36mCSV.File\u001b[39m.\n",
       "       Invalid values in \u001b[36mdrop\u001b[39m are ignored.\n",
       "\n",
       "    •  \u001b[36mlimit\u001b[39m: an \u001b[36mInteger\u001b[39m to indicate a limited number of rows to parse in\n",
       "       a csv file; use in combination with \u001b[36mskipto\u001b[39m to read a specific,\n",
       "       contiguous chunk within a file; note for large files when multiple\n",
       "       threads are used for parsing, the \u001b[36mlimit\u001b[39m argument may not result in\n",
       "       an exact # of rows parsed; use \u001b[36mthreaded=false\u001b[39m to ensure an exact\n",
       "       limit if necessary\n",
       "\n",
       "    •  \u001b[36mbuffer_in_memory\u001b[39m: a \u001b[36mBool\u001b[39m, default \u001b[36mfalse\u001b[39m, which controls whether a\n",
       "       \u001b[36mCmd\u001b[39m, \u001b[36mIO\u001b[39m, or gzipped source will be read/decompressed in memory vs.\n",
       "       using a temporary file.\n",
       "\n",
       "    •  \u001b[36mntasks::Integer=Threads.nthreads()\u001b[39m: [not applicable to \u001b[36mCSV.Rows\u001b[39m]\n",
       "       for multithreaded parsed files, this controls the number of tasks\n",
       "       spawned to read a file in concurrent chunks; defaults to the # of\n",
       "       threads Julia was started with (i.e. \u001b[36mJULIA_NUM_THREADS\u001b[39m environment\n",
       "       variable or \u001b[36mjulia -t N\u001b[39m); setting \u001b[36mntasks=1\u001b[39m will avoid any calls to\n",
       "       \u001b[36mThreads.@spawn\u001b[39m and just read the file serially on the main thread;\n",
       "       a single thread will also be used for smaller files by default (<\n",
       "       5_000 cells)\n",
       "\n",
       "    •  \u001b[36mrows_to_check::Integer=30\u001b[39m: [not applicable to \u001b[36mCSV.Rows\u001b[39m] a\n",
       "       multithreaded parsed file will be split up into \u001b[36mntasks\u001b[39m # of equal\n",
       "       chunks; \u001b[36mrows_to_check\u001b[39m controls the # of rows are checked to ensure\n",
       "       parsing correctly found valid rows; for certain files with very\n",
       "       large quoted text fields, \u001b[36mlines_to_check\u001b[39m may need to be higher\n",
       "       (10, 30, etc.) to ensure parsing correctly finds these rows\n",
       "\n",
       "    •  \u001b[36msource\u001b[39m: [only applicable for vector of inputs to \u001b[36mCSV.File\u001b[39m] a\n",
       "       \u001b[36mSymbol\u001b[39m, \u001b[36mString\u001b[39m, or \u001b[36mPair\u001b[39m of \u001b[36mSymbol\u001b[39m or \u001b[36mString\u001b[39m to \u001b[36mVector\u001b[39m. As a single\n",
       "       \u001b[36mSymbol\u001b[39m or \u001b[36mString\u001b[39m, provides the column name that will be added to\n",
       "       the parsed columns, the values of the column will be the input\n",
       "       \"name\" (usually file name) of the input from whence the value was\n",
       "       parsed. As a \u001b[36mPair\u001b[39m, the 2nd part of the pair should be a \u001b[36mVector\u001b[39m of\n",
       "       values matching the length of the # of inputs, where each value\n",
       "       will be used instead of the input name for that inputs values in\n",
       "       the auto-added column.\n",
       "\n",
       "\u001b[1m  Parsing options:\u001b[22m\n",
       "\u001b[1m  ==================\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmissingstring\u001b[39m: either a \u001b[36mnothing\u001b[39m, \u001b[36mString\u001b[39m, or \u001b[36mVector{String}\u001b[39m to use\n",
       "       as sentinel values that will be parsed as \u001b[36mmissing\u001b[39m; if \u001b[36mnothing\u001b[39m is\n",
       "       passed, no sentinel/missing values will be parsed; by default,\n",
       "       \u001b[36mmissingstring=\"\"\u001b[39m, which means only an empty field (two consecutive\n",
       "       delimiters) is considered \u001b[36mmissing\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdelim=','\u001b[39m: a \u001b[36mChar\u001b[39m or \u001b[36mString\u001b[39m that indicates how columns are\n",
       "       delimited in a file; if no argument is provided, parsing will try\n",
       "       to detect the most consistent delimiter on the first 10 rows of\n",
       "       the file\n",
       "\n",
       "    •  \u001b[36mignorerepeated::Bool=false\u001b[39m: whether repeated\n",
       "       (consecutive/sequential) delimiters should be ignored while\n",
       "       parsing; useful for fixed-width files with delimiter padding\n",
       "       between cells\n",
       "\n",
       "    •  \u001b[36mquoted::Bool=true\u001b[39m: whether parsing should check for \u001b[36mquotechar\u001b[39m at\n",
       "       the start/end of cells\n",
       "\n",
       "    •  \u001b[36mquotechar='\"'\u001b[39m, \u001b[36mopenquotechar\u001b[39m, \u001b[36mclosequotechar\u001b[39m: a \u001b[36mChar\u001b[39m (or different\n",
       "       start and end characters) that indicate a quoted field which may\n",
       "       contain textual delimiters or newline characters\n",
       "\n",
       "    •  \u001b[36mescapechar='\"'\u001b[39m: the \u001b[36mChar\u001b[39m used to escape quote characters in a\n",
       "       quoted field\n",
       "\n",
       "    •  \u001b[36mdateformat::Union{String, Dates.DateFormat, Nothing,\n",
       "       AbstractDict}\u001b[39m: a date format string to indicate how Date/DateTime\n",
       "       columns are formatted for the entire file; if given as an\n",
       "       \u001b[36mAbstractDict\u001b[39m, date format strings to indicate how the\n",
       "       Date/DateTime columns corresponding to the keys are formatted. The\n",
       "       Dict can map column index \u001b[36mInt\u001b[39m, or name \u001b[36mSymbol\u001b[39m or \u001b[36mString\u001b[39m to the\n",
       "       format string for that column.\n",
       "\n",
       "    •  \u001b[36mdecimal='.'\u001b[39m: a \u001b[36mChar\u001b[39m indicating how decimals are separated in\n",
       "       floats, i.e. \u001b[36m3.14\u001b[39m uses \u001b[36m'.'\u001b[39m, or \u001b[36m3,14\u001b[39m uses a comma \u001b[36m','\u001b[39m\n",
       "\n",
       "    •  \u001b[36mtruestrings\u001b[39m, \u001b[36mfalsestrings\u001b[39m: \u001b[36mVector{String}\u001b[39ms that indicate how \u001b[36mtrue\u001b[39m\n",
       "       or \u001b[36mfalse\u001b[39m values are represented; by default \u001b[36m\"true\", \"True\",\n",
       "       \"TRUE\", \"T\", \"1\"\u001b[39m are used to detect \u001b[36mtrue\u001b[39m and \u001b[36m\"false\", \"False\",\n",
       "       \"FALSE\", \"F\", \"0\"\u001b[39m are used to detect \u001b[36mfalse\u001b[39m; note that columns with\n",
       "       only \u001b[36m1\u001b[39m and \u001b[36m0\u001b[39m values will default to \u001b[36mInt64\u001b[39m column type unless\n",
       "       explicitly requested to be \u001b[36mBool\u001b[39m via \u001b[36mtypes\u001b[39m keyword argument\n",
       "\n",
       "\u001b[1m  Column Type Options:\u001b[22m\n",
       "\u001b[1m  ======================\u001b[22m\n",
       "\n",
       "    •  \u001b[36mtypes\u001b[39m: a single \u001b[36mType\u001b[39m, \u001b[36mAbstractVector\u001b[39m or \u001b[36mAbstractDict\u001b[39m of types, or\n",
       "       a function of the form \u001b[36m(i, name) -> Union{T, Nothing}\u001b[39m to be used\n",
       "       for column types; if a single \u001b[36mType\u001b[39m is provided, \u001b[4mall\u001b[24m columns will\n",
       "       be parsed with that single type; an \u001b[36mAbstractDict\u001b[39m can map column\n",
       "       index \u001b[36mInteger\u001b[39m, or name \u001b[36mSymbol\u001b[39m or \u001b[36mString\u001b[39m to type for a column, i.e.\n",
       "       \u001b[36mDict(1=>Float64)\u001b[39m will set the first column as a \u001b[36mFloat64\u001b[39m,\n",
       "       \u001b[36mDict(:column1=>Float64)\u001b[39m will set the column named \u001b[36mcolumn1\u001b[39m to\n",
       "       \u001b[36mFloat64\u001b[39m and, \u001b[36mDict(\"column1\"=>Float64)\u001b[39m will set the \u001b[36mcolumn1\u001b[39m to\n",
       "       \u001b[36mFloat64\u001b[39m; if a \u001b[36mVector\u001b[39m is provided, it must match the # of columns\n",
       "       provided or detected in \u001b[36mheader\u001b[39m. If a function is provided, it\n",
       "       takes a column index and name as arguments, and should return the\n",
       "       desired column type for the column, or \u001b[36mnothing\u001b[39m to signal the\n",
       "       column's type should be detected while parsing.\n",
       "\n",
       "    •  \u001b[36mtypemap::Dict{Type, Type}\u001b[39m: a mapping of a type that should be\n",
       "       replaced in every instance with another type, i.e.\n",
       "       \u001b[36mDict(Float64=>String)\u001b[39m would change every detected \u001b[36mFloat64\u001b[39m column\n",
       "       to be parsed as \u001b[36mString\u001b[39m; only \"standard\" types are allowed to be\n",
       "       mapped to another type, i.e. \u001b[36mInt64\u001b[39m, \u001b[36mFloat64\u001b[39m, \u001b[36mDate\u001b[39m, \u001b[36mDateTime\u001b[39m, \u001b[36mTime\u001b[39m,\n",
       "       and \u001b[36mBool\u001b[39m. If a column of one of those types is \"detected\", it will\n",
       "       be mapped to the specified type.\n",
       "\n",
       "    •  \u001b[36mpool::Union{Bool, Real, AbstractVector, AbstractDict,\n",
       "       Function}=0.25\u001b[39m: [not supported by \u001b[36mCSV.Rows\u001b[39m] controls whether\n",
       "       columns will be built as \u001b[36mPooledArray\u001b[39m; if \u001b[36mtrue\u001b[39m, all columns\n",
       "       detected as \u001b[36mString\u001b[39m will be pooled; alternatively, the proportion\n",
       "       of unique values below which \u001b[36mString\u001b[39m columns should be pooled (by\n",
       "       default 0.25, meaning that if the # of unique strings in a column\n",
       "       is under 25.0%, it will be pooled); if an \u001b[36mAbstractVector\u001b[39m, each\n",
       "       element should be \u001b[36mBool\u001b[39m or \u001b[36mReal\u001b[39m and the # of elements should match\n",
       "       the # of columns in the dataset; if an \u001b[36mAbstractDict\u001b[39m, a \u001b[36mBool\u001b[39m or\n",
       "       \u001b[36mReal\u001b[39m value can be provided for individual columns where the dict\n",
       "       key is given as column index \u001b[36mInteger\u001b[39m, or column name as \u001b[36mSymbol\u001b[39m or\n",
       "       \u001b[36mString\u001b[39m. If a function is provided, it should take a column index\n",
       "       and name as 2 arguments, and return a \u001b[36mBool\u001b[39m, \u001b[36mFloat64\u001b[39m, or \u001b[36mnothing\u001b[39m\n",
       "       for each column.\n",
       "\n",
       "    •  \u001b[36mdowncast::Bool=false\u001b[39m: controls whether columns detected as \u001b[36mInt64\u001b[39m\n",
       "       will be \"downcast\" to the smallest possible integer type like\n",
       "       \u001b[36mInt8\u001b[39m, \u001b[36mInt16\u001b[39m, \u001b[36mInt32\u001b[39m, etc.\n",
       "\n",
       "    •  \u001b[36mstringtype=InlineStrings.InlineString\u001b[39m: controls how detected\n",
       "       string columns will ultimately be returned; default is\n",
       "       \u001b[36mInlineString\u001b[39m, which stores string data in a fixed-size primitive\n",
       "       type that helps avoid excessive heap memory usage; if a column has\n",
       "       values longer than 32 bytes, it will default to \u001b[36mString\u001b[39m. If \u001b[36mString\u001b[39m\n",
       "       is passed, all string columns will just be normal \u001b[36mString\u001b[39m values.\n",
       "       If \u001b[36mPosLenString\u001b[39m is passed, string columns will be returned as\n",
       "       \u001b[36mPosLenStringVector\u001b[39m, which is a special \"lazy\" \u001b[36mAbstractVector\u001b[39m that\n",
       "       acts as a \"view\" into the original file data. This can lead to the\n",
       "       most efficient parsing times, but note that the \"view\" nature of\n",
       "       \u001b[36mPosLenStringVector\u001b[39m makes it read-only, so operations like \u001b[36mpush!\u001b[39m,\n",
       "       \u001b[36mappend!\u001b[39m, or \u001b[36msetindex!\u001b[39m are not supported. It also keeps a reference\n",
       "       to the entire input dataset source, so trying to modify or delete\n",
       "       the underlying file, for example, may fail\n",
       "\n",
       "    •  \u001b[36mstrict::Bool=false\u001b[39m: whether invalid values should throw a parsing\n",
       "       error or be replaced with \u001b[36mmissing\u001b[39m\n",
       "\n",
       "    •  \u001b[36msilencewarnings::Bool=false\u001b[39m: if \u001b[36mstrict=false\u001b[39m, whether invalid\n",
       "       value warnings should be silenced\n",
       "\n",
       "    •  \u001b[36mmaxwarnings::Int=100\u001b[39m: if more than \u001b[36mmaxwarnings\u001b[39m number of warnings\n",
       "       are printed while parsing, further warnings will be silenced by\n",
       "       default; for multithreaded parsing, each parsing task will print\n",
       "       up to \u001b[36mmaxwarnings\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdebug::Bool=false\u001b[39m: passing \u001b[36mtrue\u001b[39m will result in many informational\n",
       "       prints while a dataset is parsed; can be useful when reporting\n",
       "       issues or figuring out what is going on internally while a dataset\n",
       "       is parsed\n",
       "\n",
       "    •  \u001b[36mvalidate::Bool=true\u001b[39m: whether or not to validate that columns\n",
       "       specified in the \u001b[36mtypes\u001b[39m, \u001b[36mdateformat\u001b[39m and \u001b[36mpool\u001b[39m keywords are actually\n",
       "       found in the data. If \u001b[36mfalse\u001b[39m no validation is done, meaning no\n",
       "       error will be thrown if \u001b[36mtypes\u001b[39m/\u001b[36mdateformat\u001b[39m/\u001b[36mpool\u001b[39m specify settings for\n",
       "       columns not actually found in the data.\n",
       "\n",
       "\u001b[1m  Iteration options:\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "    •  \u001b[36mreusebuffer=false\u001b[39m: [only supported by \u001b[36mCSV.Rows\u001b[39m] while iterating,\n",
       "       whether a single row buffer should be allocated and reused on each\n",
       "       iteration; only use if each row will be iterated once and not\n",
       "       re-used (e.g. it's not safe to use this option if doing\n",
       "       \u001b[36mcollect(CSV.Rows(file))\u001b[39m because only current iterated row is\n",
       "       \"valid\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CSV.File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Let's consider [this dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/00504/), the content of which we saved in a file at path `fpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"\"\"\n",
    "3.26;0.829;1.676;0;1;1.453;3.770\n",
    "2.189;0.58;0.863;0;0;1.348;3.115\n",
    "2.125;0.638;0.831;0;0;1.348;3.531\n",
    "3.027;0.331;1.472;1;0;1.807;3.510\n",
    "2.094;0.827;0.86;0;0;1.886;5.390\n",
    "3.222;0.331;2.177;0;0;0.706;1.819\n",
    "3.179;0;1.063;0;0;2.942;3.947\n",
    "3;0;0.938;1;0;2.851;3.513\n",
    "2.62;0.499;0.99;0;0;2.942;4.402\n",
    "2.834;0.134;0.95;0;0;1.591;3.021\n",
    "2.405;0.134;0.843;0;0;1.769;3.210\n",
    "2.728;0.223;0.953;0;0;1.591;2.371\n",
    "2.512;0.223;0.929;1;0;1.769;3.919\n",
    "2.834;0.134;1.237;0;0;1.859;3.030\n",
    "2.819;0.331;1.271;0;1;0.981;2.736\n",
    "2.126;0.251;1.114;0;0;0.143;2.157\n",
    "2.834;0.134;1.322;0;0;1.199;2.413\n",
    "3.014;0.56;1.781;0;0;-0.115;0.898\n",
    "3.024;0.452;2.698;0;0;1.107;0.450\n",
    "3.036;0.405;1.205;1;0;1.807;3.733\n",
    "2.707;0.972;1.889;0;3;-1.169;2.976\n",
    "2.978;1.246;1.103;0;1;3.988;6.535\n",
    "3.111;0.732;0.923;0;0;4.068;5.643\n",
    "\"\"\"\n",
    "fpath, = mktemp()\n",
    "write(fpath, c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't have a header so we have to provide it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>3 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>CIC0</th><th>SM1_Dz</th><th>GATS1i</th><th>NdsCH</th><th>NdssC</th><th>MLOGP</th><th>LC50</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>3.26</td><td>0.829</td><td>1.676</td><td>0</td><td>1</td><td>1.453</td><td>3.77</td></tr><tr><th>2</th><td>2.189</td><td>0.58</td><td>0.863</td><td>0</td><td>0</td><td>1.348</td><td>3.115</td></tr><tr><th>3</th><td>2.125</td><td>0.638</td><td>0.831</td><td>0</td><td>0</td><td>1.348</td><td>3.531</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& CIC0 & SM1\\_Dz & GATS1i & NdsCH & NdssC & MLOGP & LC50\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Int64 & Int64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.26 & 0.829 & 1.676 & 0 & 1 & 1.453 & 3.77 \\\\\n",
       "\t2 & 2.189 & 0.58 & 0.863 & 0 & 0 & 1.348 & 3.115 \\\\\n",
       "\t3 & 2.125 & 0.638 & 0.831 & 0 & 0 & 1.348 & 3.531 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m CIC0    \u001b[0m\u001b[1m SM1_Dz  \u001b[0m\u001b[1m GATS1i  \u001b[0m\u001b[1m NdsCH \u001b[0m\u001b[1m NdssC \u001b[0m\u001b[1m MLOGP   \u001b[0m\u001b[1m LC50    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────\n",
       "   1 │   3.26     0.829    1.676      0      1    1.453    3.77\n",
       "   2 │   2.189    0.58     0.863      0      0    1.348    3.115\n",
       "   3 │   2.125    0.638    0.831      0      0    1.348    3.531"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"CIC0\", \"SM1_Dz\", \"GATS1i\",\n",
    "          \"NdsCH\", \"NdssC\", \"MLOGP\", \"LC50\"]\n",
    "data = CSV.read(fpath, DataFrames.DataFrame, header=header)\n",
    "first(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Let's consider [this dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/00423/), the content of which we saved at `fpath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"\"\"\n",
    "1,0,1,0,0,0,0,1,0,1,1,?,1,0,0,0,0,1,0,0,0,0,1,67,137,15,0,1,1,1.53,95,13.7,106.6,4.9,99,3.4,2.1,34,41,183,150,7.1,0.7,1,3.5,0.5,?,?,?,1\n",
    "0,?,0,0,0,0,1,1,?,?,1,0,0,1,0,0,0,1,0,0,0,0,1,62,0,?,0,1,1,?,?,?,?,?,?,?,?,?,?,?,?,?,?,1,1.8,?,?,?,?,1\n",
    "1,0,1,1,0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,78,50,50,2,1,2,0.96,5.8,8.9,79.8,8.4,472,3.3,0.4,58,68,202,109,7,2.1,5,13,0.1,28,6,16,1\n",
    "1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,1,1,77,40,30,0,1,1,0.95,2440,13.4,97.1,9,279,3.7,0.4,16,64,94,174,8.1,1.11,2,15.7,0.2,?,?,?,0\n",
    "1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,1,76,100,30,0,1,1,0.94,49,14.3,95.1,6.4,199,4.1,0.7,147,306,173,109,6.9,1.8,1,9,?,59,15,22,1\n",
    "1,0,1,0,?,0,0,1,0,?,0,1,0,0,0,0,0,1,1,1,0,0,1,75,?,?,1,1,2,1.58,110,13.4,91.5,5.4,85,3.4,3.5,91,122,242,396,5.6,0.9,1,10,1.4,53,22,111,0\n",
    "1,0,0,0,?,1,1,1,0,0,1,0,?,0,0,0,0,0,0,0,0,0,1,49,0,0,0,1,1,1.4,138.9,10.4,102,3.2,42000,2.35,2.72,119,183,143,211,7.3,0.8,5,2.6,2.19,171,126,1452,0\n",
    "1,1,1,0,?,0,0,1,0,1,1,?,0,0,0,0,0,0,1,1,1,0,1,61,?,20,3,1,1,1.46,9860,10.8,92,3,58,3.1,3.2,79,108,184,300,7.1,0.52,2,9,1.3,42,25,706,0\n",
    "1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,?,1,1,0,0,1,50,100,32,1,1,2,3.14,8.8,11.9,107.5,4.9,70,1.9,3.3,26,59,115,63,6.1,0.59,1,6.4,1.2,85,73,982,1\n",
    "1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,43,100,0,0,1,1,1.12,1.8,11.8,87.8,5100,193000,4.2,0.5,71,45,256,303,7.1,0.59,1,9.3,0.7,?,?,?,1\n",
    "1,0,1,0,0,0,1,1,?,?,0,0,0,0,0,0,0,?,1,1,0,0,1,41,?,?,0,1,2,1.05,100809,13,94.2,5.7,196,4.4,3,90,334,494,236,7.6,0.8,5,?,1.1,?,?,?,0\n",
    "1,0,1,0,0,0,1,1,1,0,0,0,0,1,0,0,0,?,0,1,0,0,1,74,?,0,0,1,1,1.33,86,15.7,96.7,4,61,3.7,1.3,132,168,113,154,?,7.6,5,1.9,0.3,144,41,277,1\n",
    "1,0,1,0,0,0,0,1,0,1,1,0,0,1,0,0,?,?,1,1,1,0,0,66,?,30,0,1,1,1.53,60,13.3,90.1,5.5,207000,4.4,8.5,25,36,35,74,8.5,0.73,1,5,0.8,?,?,?,1\n",
    "1,?,0,0,0,0,1,1,?,?,0,0,0,0,0,0,0,0,0,0,0,0,1,56,0,?,0,1,1,1.2,6.6,13.7,93.8,4.1,91000,4.5,1,103,96,205,70,8.8,0.88,1,22,?,82,24,?,1\n",
    "1,0,1,0,0,0,0,1,0,?,1,0,0,1,0,0,?,1,1,1,0,0,1,63,?,?,2,2,2,1.25,29,13.5,93,6,128,3.15,10.5,76,116,165,163,7.3,1.07,4,4.5,4.5,197,84,302,1\n",
    "0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,41,100,0,1,1,2,1.61,4.6,10.2,89.6,5.5,161,3.1,3.1,24,57,163,176,5,0.8,2,2.6,1.3,25,13,60,1\n",
    "1,0,1,0,0,0,0,1,?,1,1,?,1,0,0,0,?,?,1,1,1,0,1,72,?,?,3,2,1,2.14,60,12.1,99.2,5,58,2.4,9.8,69,63,201,235,6.2,0.96,2,2,2.9,136,95,767,0\n",
    "1,1,1,0,0,0,0,1,0,1,0,0,?,0,0,0,?,1,1,1,1,1,1,60,100,60,2,1,1,1.05,9.2,10.3,103.7,5.4,159,3.8,0.5,56,91,459,146,5.4,1.23,5,13.5,3.8,187,58,443,1\n",
    "1,?,1,0,0,0,0,1,?,1,0,?,0,1,1,0,0,1,0,0,0,0,1,64,200,78,1,1,1,1.13,8.8,14.9,94.8,6.3,137,4.3,0.9,16,23,82,180,6.5,4.95,1,5.4,0.9,144,49,295,1\n",
    "1,1,1,0,0,0,0,1,?,?,0,0,0,1,0,0,0,1,1,1,1,0,1,75,500,?,0,1,3,1.44,34,15.9,103.4,9600,101000,3.4,3.4,27,87,260,147,6.3,0.9,5,2.3,1.6,67,34,774,0\n",
    "\"\"\"\n",
    "fpath, = mktemp()\n",
    "write(fpath, c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not have a header and missing values indicated by `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>3 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>Column2</th><th>Column3</th><th>Column4</th><th>Column5</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Union{Missing, Int64}\">Int64?</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Union{Missing, Int64}\">Int64?</th></tr></thead><tbody><tr><th>1</th><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><th>2</th><td>0</td><td><em>missing</em></td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64? & Int64 & Int64 & Int64?\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 0 & 1 & 0 & 0 \\\\\n",
       "\t2 & 0 & \\emph{missing} & 0 & 0 & 0 \\\\\n",
       "\t3 & 1 & 0 & 1 & 1 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Int64?  \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Int64?  \u001b[0m\n",
       "─────┼─────────────────────────────────────────────\n",
       "   1 │       1        0        1        0        0\n",
       "   2 │       0 \u001b[90m missing \u001b[0m       0        0        0\n",
       "   3 │       1        0        1        1        0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(fpath, DataFrames.DataFrame, header=false, missingstring=\"?\") # ?로 된 string은 missing으로 처리하라는 의미 \n",
    "first(data[:, 1:5], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
